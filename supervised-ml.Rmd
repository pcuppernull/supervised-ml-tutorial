---
title: "Supervised ML Tutorial"
author: "Pete Cuppernull"
date: "2/26/2021"
output: pdf_document
---

# What is Supervised Learning?

Supervised learning uses both the inputs and outputs of a data generating process to teach a computer to predict the value of an outcome variable. As a researcher, you provide the computer with both the values of the independent variables (the inputs) and the dependent variable (the outputs), and the computer figures out how that inputs relate to each other to produce the outputs. Once this model -- or _learner_ -- has been "trained" by your data, you can apply it to new data to generate predictions. 

Unlike unsupervised learning, which does not require the outputs to train a model, supervised learning requires output values. Often, this means a researcher must "hand code" a subset of their data to provide the accurate dependent variable values to the computer.

There are a handful of concepts and vocab terms that will be useful for supervised learning:

  - Accuracy -- The accuracy of a learner can be evaluated in many ways. For _classification_ problems, where the learner seeks to delineate between predetermined outcome categories, we can often evaluate the learner's accuracy by the percentage of the observations it places into the correct category (this is what we will do in the examples below). For _regression_ problems, where the outcome variable is continuous and there is not a clear distinction between a "correct" and "incorrect" classification, mean squared error is a popular validation choice (there are many others too).  
  
  - Interpretability -- some types of learners (like a linear regression model) are interpretable, meaning we as researchers can understand how the model makes predictions. Others, like neural nets, are "black box" models where it is very difficult to determine how the model makes its choices. If you value interpretability, sometimes this means choosing a less accurate learner.
  
  - Bias-Variance Tradeoff -- it is possible to "overfit" a learner. This is the scenario where the learner gets very good at predicting your training data, but fails to generalize to accurately predict new data! In this case, we have a low bias, but high variance model. We usually want to maximize the out-of-sample prediction accuracy. This means inducing some bias in the model (intentionally getting some predictions wrong) in order to lower the variance, and hopefully achieve greater out-of-sample prediction accuracy. 
  
  - Hyperparameters -- these are macro level settings for the model that usually vary by model type.


# Code

## Load Libraries

I will be using the `caret` library below. This is a good one-stop-shop package for many ML tasks. It relies on several other packages under the hood, so when you first run the code, it may ask you to install some extra packages.

```{r}
library(tidyverse)
library(caret)
```

## Import Data

I am using some data from the most recent ANES pilot survey.

```{r}
data <- read_csv("data/anes_pilot_2020ets_csv.csv")

head(data)
```

## Clean Data

I'm going to select some variables to work with. Our outcome variable, `violence_ok`, records a respondent's answer to the following question: 

  "How much do you feel it is justified for people to use violence to pursue their political goals in this country?"
  
If the respondent answered "Not at all", the variable is coded as 0. All other responses ("A little", "A moderate amount", "A lot", "A great deal") are coded as 1. 

For independent variables (the inputs, or _predictors_), I've selected a variety of feeling thermometers (towards presidential candidates, political parties, etc) as well as some questions on economic policy preferences.

```{r}
data_clean <- data %>%
  select(viol2a, fttrump1:ftdemocraticparty, econnow:ineqinc1a) # select relevant variables

data_clean[data_clean == 999] <- NA #replace "999" ("I don't know" responses) with NAs

data_clean <- data_clean %>%
  na.omit() %>% #remove rows with NAs
  mutate(violence_ok = as.factor(if_else(viol2a == 1, 0, 1))) %>% #convert original variable to binary response, as described above
  select(-viol2a, -billtax2, -guarinc2, -freemkt2, -freemkt3, -govsize2, -govsize3, -regulate2) #drop some columns

head(data_clean)
```

## Set Seed and Split into Training and Testing

Setting the seed is important because we will do a lot of random sampling for supervised learning. This makes our results reproducible.

After setting the seed, we split our data into _Training_ and _Testing_ sets. We will use the Training data to train the model. The Testing data we will use as a final validation step to test our out-of-sample prediction accuracy.

Keeping 80% of the data in training and 20% in testing is a common approach. This can vary however -- and 80/20 is not a hard rule.

```{r}
set.seed(1414)

sample <- sample(nrow(data_clean), nrow(data_clean)*.8) #choose rows from data_clean to sample
training <- data_clean[sample,] #save those rows as training
testing <- data_clean[-sample,] #add all the other rows to testing

training %>%
  count(violence_ok) #about 65% of respondents condone some level of political violence
```


## Learner Types

I am going to demo three different types of learners. There are literally hundreds of possibilities to choose from (check out the `caret` documentation for a full list). One approach to supervised learning is to test out a bunch of different learners, see which is the most accurate, and then roll with that one!

### Logit

A logit model is a common choice of a learner for a binomial choice outcome (like we have in this example: it's either 1, or it's 0). It is also relatively interpretable, which can be beneficial.

For `caret`, you can set some macro options for the models. Here, I am opting to use "10 Fold Cross Validation". This is a technique that helps you balance the bias-variance tradeoff in the estimated models.

```{r}
fitControl <- trainControl(method = "cv",
             number = 10)
```

Next, we fit the model. 

```{r}
logit_fit <- train(violence_ok ~ ., #regress violence_ok onto all other predictors, which is what the "." means
                   method = "glm",
                   family = "binomial",  #method and family here specify a logit model
                  data = training, #using our training data
                  trControl = fitControl) #add our macro parameters

logit_fit

```

The "Accuracy" measure is from the cross validation procedure. This gives us a glimpse at the model's accuracy, but doesn't tell us the accuracy on the testing data. I evaluate the prediction accuracy on the testing data below. I use the model to generate new predictions, compare them to the "true" values from the testing data, and see how often we got the prediction right.

```{r}
1 - sum(abs((as.numeric(predict(logit_fit, testing)) - 1) - (as.numeric(testing$violence_ok) - 1))) / nrow(testing)
```

So, the logit model makes the correct prediction almost 88% of the time on the testing data. Not too bad!

### Elastic Net

Elastic Net is another common learner type. Two hyperparameters of elastic net models that are worth tuning are "Alpha" and "Lambda" - `Caret` automatically runs several iterations of the elastic net model with different values for each. It then shows you the accuracy at each level and will suggest the most accurate model.

```{r}
elastic_fit <- train(violence_ok ~ ., #same as before
                   method = "glmnet", #specify the model type here
                   data = training, #same as before
                  #tuneLength = 10 # You can use this argument to test more options of hyperparameter values
                  trControl = fitControl) #same as before

elastic_fit

elastic_fit$results %>%
  arrange(-Accuracy) #order by the most accurate models

```

#### Apply Elastic Net Models to Holdout Data

We're going to do some "hyperparameter tuning", where we test out different values of alpha and lambda, and evaluate each model's accuracy on the testing data. The code below makes it a little easier to generate predictions and plot the out-of-sample accuracy.

```{r}
#Set Range of hyperparameters alpha and lambda. expand.grid creates a DF with a row for each unique combination of hyperparameter values.

elastic_grid <- expand.grid(.alpha = seq(0, 1, .2),
                            .lambda = c(.001, .1, 1))


elasticFunction <- function(row) { #function to train the model. This is basically the same as above, but only runs 1 set of hyperparameter values at a time.
  train(violence_ok ~ .,
                   method = "glmnet",
                   data = training, 
                   trControl = fitControl,
                   tuneGrid = elastic_grid[row,])
}

###This next step will probably take a few minutes for your computer to run!!
elastic_models <- map(1:nrow(elastic_grid), elasticFunction) #run the above function for each set of hyperparameter values and save results as a list

elastic_accuracy <- c() # create an empty vector to store the results of the accuracy test below

for (i in 1:length(elastic_models)){ #"for loop" to take each model, generate predictions, compare them to the true values, and save the results

elastic_accuracy[i] <- 1 - sum(abs((as.numeric(predict(elastic_models[i], testing)[[1]]) - 1) - (as.numeric(testing$violence_ok) - 1))) / nrow(testing)
  
}

## Results
cbind(as.data.frame(elastic_accuracy), elastic_grid) %>%
  arrange(-elastic_accuracy)
```


### kNN

K Nearest Neighbors, or "kNN", is another common learner type. To make a classification in kNN, the model looks for the k most similar observations in the training set. Whichever is the most popular outcome of the k most similar training observations is designated as the prediction for the test observation.

Choosing k is tricky. k of 1 might be too low because you are making a classification based on a single training observation! A k value that is too large might lose some of the nuance in the data by not looking close enough.

Here, it is important to do some hyperparameter tuning. We will set a range of possible k values, train a model for each of them, apply each model to the testing data, and see which does best.

Last thing: for kNN, since we are evaluating the "closest" training observations, we need to make the scales of all the independent variables comparable. Otherwise, some variables would be represented differently in space than others. We normalize the variables by reshaping each to have mean 0 and standard deviation of 1.

```{r}
x_scaled <- data_clean %>%
  select(-violence_ok) %>%
  scale() %>%
  as.data.frame()

data_knn <- cbind(data_clean$violence_ok, x_scaled) %>%
  rename(violence_ok = `data_clean$violence_ok`)

training_knn <- data_knn[sample,] #save those rows as training
testing_knn <- data_knn[-sample,]

```

Now, we train the model.

```{r}
knn_grid <- expand.grid(k = seq(1, 100, 5)) 

knn_fit <- train(violence_ok ~ .,
                   method = "knn",
                   data = training_knn, 
                   trControl = fitControl,
                   tuneGrid = knn_grid) #add tuneGrid to put in the range of k values we jjust created

knn_fit
```

#### Apply kNN Models to Holdout Data

Similar to the elastic net example, below is some extra code which will create a model for each k value and test its accuracy on the testing data. We can then plot the results.

```{r}
knnFunction <- function(k) { #function to train the model. This is basically the same as above, but only runs 1 k value at a time.
  
  knn_grid <- expand.grid(k = k)
  
  train(violence_ok ~ .,
                   method = "knn",
                   data = training_knn, 
                   trControl = fitControl,
                   tuneGrid = knn_grid)
}

knn_models <- map(seq(1, 101, 5), knnFunction) #run the above function for each k value and save results as a list

test_accuracy <- c() # create an empty vector to store the results of the accuracy test below

for (i in 1:length(knn_models)){ #for loop to take each model, generate predictions, compare them to the true values, and save the results

test_accuracy[i] <- 1 - sum(abs((as.numeric(predict(knn_models[i], testing_knn)[[1]]) - 1) - (as.numeric(testing_knn$violence_ok) - 1))) / nrow(testing_knn)
  
}

##Plot Results
as.data.frame(test_accuracy) %>% #save the results as a dataframe
  mutate(k = seq(1, 101, 5)) %>% #add the k values as a column
  ggplot(aes(k, test_accuracy)) +
  geom_line() +
  labs(x = "K Nearest Neighbors",
       y = "Prediction Accuracy",
       title = "Prediction Accuracy of kNN Learner on Holdout Data")
```


